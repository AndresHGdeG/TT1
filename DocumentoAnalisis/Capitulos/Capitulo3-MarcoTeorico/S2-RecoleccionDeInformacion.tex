
\MTtitle{Recolección de información de Internet} 

\section{Web Scraping}
La recopilación de datos de Internet es una técnica que se realiza de manera manual, sin embargo 
el \textit{Web Scraping} es el conjunto de técnicas utilizadas para obtener de manera automática información de 
un sitio web en lugar de recopilarla manualmente \citep{CTWebScraping}. 
%El objetivo del Web Scraping es buscar cierto tipo de información definida y agregarla a nuevas páginas web. 
\\
El \textit{Web Scraping} accede a las páginas web, encuentra los elementos de datos especificados en la 
página, los extrae, los transforma en diferentes formatos si es necesario y finalmente, guarda 
estos datos como un conjunto de datos estructurado\footnote{Un conjunto de datos estructurado permite recolectar 
varios valores simultaneamente.}. Los investigadores utilizan este contenido para analizar los datos 
después de limpiar y organizar lo datos.
\subsection{Técnicas de Web Scraping}

Algunas de las técnicas que nos proporciona el \textit{Web Scraping son}:
\begin{itemize}
    \item \textbf{Copiar y pegar}: Realiza el método recolección copiar y pegar la información, 
    sin embargo es una técnica propensa a errores
    \item \textbf{Uso de expresiones regulares}: Es una técnica que se puede utilizar para obtener la información 
    de las páginas web son las expresiones regulares, aunque comúnmente no se recomienda utilizarlas para parsear el formato HTML.
    \item \textbf{Reconocimiento de anotaciones semánticas}: Las páginas que deben pueden contener metadatos, 
    marcas semánticas o explicaciones adicionales que se pueden usar para encontrar fragmentos de datos específicos.
    \item \textbf{Parsers de HTML}: Algunos lenguajes, como XQuery y HTQL pueden ser utilizados para parsear documentos, recuperar 
    y transformar el contenido de documentos HTML.
\end{itemize}
En el presente trabajo serán utilizadas las técnicas de Web Scraping.
La Figura \textbf{\ref{fig:procesos}}

\begin{figure}[H]
    \centering
    \includegraphics[scale=.35]{imagenes/Capitulo5/procesos}
    \caption{Etapas del proceso de \textit{Web Scraping}.}
    \label{fig:procesos}
  \end{figure}
  

\subsection{Marco legal}
La recuperación automática de datos de la Web \textit{Web Scraping} para proyectos de investigación académicos y de la 
industria se está convirtiendo en una práctica común. Se han desarrollado una variedad de herramientas y tecnologías 
para facilitar el textit{Web Scraping}, sin embargo el \textit{Web Scraping} pudiera ir en contra de los términos legales 
de uso de ciertos sitios web, la duplicación de expresiones originales puede ser un caso ilegal \citep{CTMarco}.


\section{Crawler}
Un crawler \citep{CT14} es una herramienta la cual analiza sitios web, permitiendo recolectar 
las páginas web para así posteriormente extraer la información que contengan. Un crawler también 
conocido como como robot o spider, es un sistema para la descarga masiva de páginas web. Son uno de 
los componentes principales de los motores de búsqueda web, los sistemas que reúnen un conjunto de 
páginas web, las indexan y permiten a los usuarios realizar consultas contra el índice y encontrar las 
páginas web que coincidan con las consultas.

\section{Python}
\textit{Python}\footnote{http://www.python.org/} un lenguaje de programación creado en 1991, se ha convetido en uno de los más 
importantes lenguajes de programación para la ciencia de la datos, el aprendizaje automático y el desarrollo general de software en 
el mundo académico y la industria. 
En los últimos años, el soporte mejorado de Python para bibliotecas (como pandas y scikit-learn) lo ha convertido en una opción popular 
para las tareas de análisis de datos. Combinado con la solidez general de Python para la ingeniería de software de propósito general, 
es una excelente opción como idioma principal para crear aplicaciones de datos \citep{CTPython}.

\subsection{Scrapy}
\textit{Scrapy} es un \textit{framework} para rastrear sitios web y extraer datos estructurados que pueden utilizarse para una amplia 
gama de aplicaciones útiles, como la extracción de datos, el procesamiento de información o el archivo histórico.
A pesar de que Scrapy fue diseñado originalmente para el \textit{Web Scraping}, también se puede usar para extraer datos mediante 
API (como los Servicios web de Amazon Associates) \citep{CTScrapy}.
La arquitectura del proyecto Scrapy se basa en "spiders", que son rastreadores independientes que reciben un conjunto de instrucciones,
hace que sea más fácil construir y escalar grandes proyectos de \textit{Crawler} al permitir que los desarrolladores reutilicen su 
código. Scrapy también proporciona un una terminal de rastreo web, que los desarrolladores pueden usar para probar sus suposiciones 
sobre el comportamiento de un sitio.
