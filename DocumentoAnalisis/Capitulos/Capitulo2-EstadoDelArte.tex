
\chapter{Estado del arte}

\section{Introducción}

A continuación, se mostrarán distintos trabajos nacionales e internacionales, así como herramientas las cuales desempeñan un trabajo similar al propuesto por nosotros.

\section{Trabajos nacionales}

\begin{large}
	 \textbf{Clasificación Automática de Textos de Desastres Naturales en México}\\
\end{large}

En este trabajo se propone clasificar noticias del ámbito \'Desastres Naturales\' utilizando estrategias de reducción de dimensionalidad conocidas como umbral en la frecuencia y ganancia en la información, los métodos de clasificación utilizados fueron el clasificador simple de Bayes y vecinos más cercanos.\\

Se utilizaron 375 noticias del periódico "Reforma"como conjunto de entrenamiento, para posteriormente clasificarlas (relevantes e irrelevantes), de los cuales el 11.5 de noticias eran relevantes y el 88.5\% restante eran irrelevantes.Una vez obtenido el conjunto de noticias se procedió con un pre-procesamiento, el cual reduciía el tamaño de los documentos, eliminando las partes de los textos que no se consideraban relevantes; posteriormente se realizó el indexado, el cual los documentos son representdos por vectores de palabras en un espacio de dimensionalidad n en el cual se logró una reducción de dimensionalidad en donde finalemente se utilizarón técnicas de clasificación como el algoritmo simple de Bayes en el cual se obtuvo un resultado del 97\% de efectividad al clasifciar noticias de desastres naturales [4].\\

\begin{large}
	 \textbf{Clasificación de Texto Mediante Atributos Probabilísiticos de Concurrencia de Palabras}\\
\end{large}
	
En este trabajo se propone clasificar artículos en dos categorías (relevantes y no relevantes), los cuales se encuentran almacenados en una base de datos llamada Regulon DB, la cual contiene más de 2,000 artículos clasificados manualmente relacionados a los genes de regulación de la bacteria Escherichia coli; con el objetivo de facilitar la búsqueda de información acerca de dicha bacteria.
Proponen medir los atributos es decir las palabras de un documento, utilizando representaciones probabilísticas utilizadas en el entorno de análisis de información.
Se utilizarón 1,823 documentos; en la etapa de clasificación se entrenaron a  los clasificadores Bayes simple y Bayes multinominal; obtiendo mejores resultados con el método Bayes Naive [5].


\section{Trabajos internacionales}


\begin{large}
	 \textbf{Clasificación Automática de Textos Usando Redes de Palabras}\\
\end{large}

En este trabajo se propone un algoritmo para la clasificación automática de textos basado en una representación y clasificación distinta utilizada en los algoritmos de clasificación supervisada, utilizando redes de palabras.
Se utilizaron 1000 mensajes de texto de la plataforma Twitter, en el idioma español y correspondiente a distintos contextos, para posteriormente clasificar el tipo de contenido de los mensajes (positivos, negativos y neutrales), se definió un grafo como aquella red de palabras co-cocurrentes construida a partir de un conjunto de textos clasificados; para su realización el primero de estos procesos es llevar distintas variantes de una misma palabra a su raíz, esto para reducir la variabilidad del lenguaje posteriormente se considera las palabras plurales (terminadas con ‘s’ o ‘es’). A estas se les elimina el sufijo para compararlas con su equivalente singular, realizando el cambio de manera automática.
Los resultados mostraron que el clasificador presenta un 80\% cercanía respecto a la clasificación realizada por una persona; su nivel de desempeño fue mayor al obtenido con el algoritmo Naive Bayes [6].\\


\begin{large}
	 \textbf{Document Classification for Newspaper Articles}\\
\end{large}

En este trabajo se enfocaron en clasificar articulos del MIT (Massachusetts Institute of Technology) de las siguientes categorías:

	\begin{itemize}
		\item Arts
		\item Features
		\item News
		\item Opinion
		\item Sports
		\item World
	\end{itemize}
Para los cuales utilizarón el algoritmos de clasificación como el Naive Bayes ya que era uno de los clasificadores más simples y eficaces que otras técnicas de clasificación, de igual manera utilizarón la clasificación máxima de entropia el cual provee segmentación de texto, modelado de lenguaje.
Se utilizarón un corpus 3000 artículos en total, siendo 500 artículos de cada sección mencionada, para el entrenamiento se utilizarón 120 artículos siendo 20 de cada sección y teniendo como resultado un 77\% de exactitud [7].


\section{Herramientas disponibles}


Entre las herramientas utilizadas para el procesamiento de lenguaje natural y aprendizaje automático se encuentran:\\

\begin{large}
	 \textbf{Cloud Natual Language}\\
\end{large}

Google Cloud Natural Language [8] revela la estructura y el significado del texto con modelos potentes de aprendizaje automático previamente entrenados en una API de REST fácil de usar y con modelos personalizados se puede utilizar para extraer información sobre personas, lugares, eventos y muchos otros datos, que se mencionan en documentos de texto, artículos periodísticos o entradas de blog. También puedes utilizarla para comprender las opiniones sobre tus productos expresadas en los medios sociales o analizar la intención en las conversaciones de los clientes que se den en un centro de atención telefónica o una aplicación de mensajería.\\

\begin{large}
	 \textbf{GoogleBot}\\
\end{large}

El crawler más famoso del mundo es Googlebot, el software diseñado por Google para indexar el contenido nuevo o actualizado de Internet.
Googlebot [9] no sólo tiene la capacidad de rastrear e indexar los sitios web de internet, sino que además puede extraer información de ficheros como pueden ser PDF, XLS, DOC, etc.
Una vez el contenido está indexado, el servidor lo clasifica y establece un orden de relevancia para las distintas búsquedas que pueda efectuar un usuario, es decir, lo posiciona.\\



\begin{large}
	 \textbf{Watson Natural Language Classifier}\\
\end{large}

Watson NLC [10] aplica técnicas de computación cognitiva para analizar un texto y proporcionar la clase que mejor encaja entre un conjunto de clases predefinidas a partir de un texto corto.
Al ser un clasificador, esta compuesto de ciertos pasos, en primera instancia se necesitan de clases las cuales son etiquetas que identificarán el texto analizado y será la salida proporcionada por el clasificador; posteriormente se debe tomar en cuenta que se necesita de una colección de textos, los cuales proporcionarán apoyo para que el clasificador logre identificar las clases ingresadas posteriormente teniendo todos estos datos se logra entrenar al clasificador, el cual proporcionará una salida dependiendo a los datos que fueron utilizados.


